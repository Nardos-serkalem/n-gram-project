{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls /content"
      ],
      "metadata": {
        "id": "WazRwwKGTlEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f68429-c97b-4e3c-a404-b422f4afc4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  yoda-corpus.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZtLo8iik6an",
        "outputId": "da2887bf-08e2-405a-88c2-2fbdcc08c884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n",
        "import os\n",
        "\n",
        "os.listdir('/content')\n",
        "os.chdir('/content')\n",
        "\n",
        "def read_csv(file_path, delimiter=','):\n",
        "       header = []\n",
        "       data = []\n",
        "       if os.path.exists(file_path):\n",
        "           with open(file_path, 'r', encoding='utf-8') as file:\n",
        "               if file.readline():\n",
        "                   header = file.readline().strip().split(delimiter)\n",
        "\n",
        "               for line in file:\n",
        "                   row = line.strip().split(delimiter)\n",
        "                   data.append(row)\n",
        "\n",
        "       return header, data\n",
        "\n",
        "\n",
        "file_path = '/content/yoda-corpus.csv'\n",
        "if os.path.exists(file_path):\n",
        "    header, data = read_csv(file_path)\n",
        "    print(\"Header:\", header)\n",
        "    print(\"Data sample:\", data[:5])\n",
        "\n",
        "\n",
        "def preprocess_yoda_text(file_path):\n",
        "    yoda_text_lines = []\n",
        "\n",
        "    try:\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip().lower()\n",
        "                yoda_text_lines.append(line)\n",
        "    except FileNotFoundError:\n",
        "      print(f\"Error: File '{file_path}' not found. please check the file path\")\n",
        "      return []\n",
        "    return yoda_text_lines\n",
        "\n",
        "file_path = 'yoda_corpus.csv'\n",
        "yoda_text_lines = preprocess_yoda_text(file_path)\n",
        "print(yoda_text_lines)\n",
        "\n",
        "def extract_text_from_csv(data, header, text_column_name):\n",
        "    if text_column_name not in header:\n",
        "        print(\"Error: 'text' column not found in the header.\")\n",
        "        return None\n",
        "\n",
        "    text_column_index = header.index(text_column_name)\n",
        "    text_data = ' '.join(row[text_column_index] for row in data)\n",
        "\n",
        "    return text_data\n",
        "\n",
        "    file_path = '/content/yoda_corpus.csv'\n",
        "header, data = read_csv(file_path)\n",
        "\n",
        "\n",
        "# Extract the text data from the column\n",
        "text_column_name = 'text'\n",
        "corpus = extract_text_from_csv(data, header, text_column_name)\n",
        "\n",
        "if corpus:\n",
        "\n",
        "# Tokenize\n",
        "   tokens = tokenize(corpus)\n",
        "   print(\"Tokens:\", tokens[:50])\n",
        "\n",
        "# generate ngram\n",
        "   bigrams = generate_ngrams(tokens, 2)\n",
        "   trigrams = generate_ngrams(tokens, 3)\n",
        "\n",
        "   print(\"Bigrams:\", bigrams[:10])\n",
        "   print(\"Trigrams:\", trigrams[:10])\n",
        "\n",
        "\n",
        "def count_ngrams(list_of_ngrams):\n",
        "   # count the ngram frequencies\n",
        "    ngram_counts = {}\n",
        "    for ngram_list in list_of_ngrams:\n",
        "        for ngram in ngram_list:\n",
        "            if ngram in ngram_counts:\n",
        "                ngram_counts[ngram] += 1\n",
        "            else:\n",
        "                ngram_counts[ngram] = 1\n",
        "    return ngram_counts\n",
        "\n",
        "def generate_ngrams(tokens, n):\n",
        "\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = ' '.join(tokens[i:i + n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "yoda_text_lines = [\n",
        "    \"Help you I can, yes.\",\n",
        "    \"The force is strong with you.\",\n",
        "    \"Pass on what you have learned.\",\n",
        "    \"Much to learn you still have.\"\n",
        "]\n",
        "\n",
        "standard_english_text = [\n",
        "    \"You must unlearn what you have learned.\",\n",
        "    \"Do or do not, there is no try.\",\n",
        "    \"The greatest teacher, failure is.\",\n",
        "    \"Adventure. Excitement. A Jedi craves not these things.\",\n",
        "    \"Fear is the path to the dark side.\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char if char.isalnum() or char.isspace() else '' for char in text])\n",
        "    return text.split()\n",
        "\n",
        "preprocessed_standard_english = [preprocess_text(line) for line in standard_english_text]\n",
        "preprocessed_yoda_lines = [preprocess_text(line) for line in yoda_text_lines]\n",
        "\n",
        "yoda_bigrams = [generate_ngrams(words, 2) for words in preprocessed_yoda_lines]\n",
        "yoda_trigrams = [generate_ngrams(words, 3) for words in preprocessed_yoda_lines]\n",
        "\n",
        "\n",
        "\n",
        "# Count the frequencies of the bigrams and trigrams\n",
        "bigram_frequencies = count_ngrams(yoda_bigrams)\n",
        "trigram_frequencies = count_ngrams(yoda_trigrams)\n",
        "\n",
        "# Generate bigrams and trigrams for Standard English and count them\n",
        "standard_english_bigrams = [generate_ngrams(words, 2) for words in preprocessed_standard_english]\n",
        "standard_english_trigrams = [generate_ngrams(words, 3) for words in preprocessed_standard_english]\n",
        "standard_bigram_frequencies = count_ngrams(standard_english_bigrams)\n",
        "standard_trigram_frequencies = count_ngrams(standard_english_trigrams)\n",
        "\n",
        "\n",
        "def compare_ngrams(ngram_frequencies1, ngram_frequencies2, ngram_type):\n",
        "\n",
        "    print(f\"Unique {ngram_type}s in the first set:\")\n",
        "    for ngram, count in ngram_frequencies1.items():\n",
        "        if ngram not in ngram_frequencies2:\n",
        "            print(f\" - {ngram}: {count}\")\n",
        "\n",
        "    print(f\"\\nUnique {ngram_type}s in the second set:\")\n",
        "    for ngram, count in ngram_frequencies2.items():\n",
        "        if ngram not in ngram_frequencies1:\n",
        "            print(f\" - {ngram}: {count}\")\n",
        "\n",
        "print(\"Comparing Bigrams:\")\n",
        "compare_ngrams(bigram_frequencies, standard_bigram_frequencies, \"bigram\")\n",
        "\n",
        "print(\"\\nComparing Trigrams:\")\n",
        "compare_ngrams(trigram_frequencies, standard_trigram_frequencies, \"trigram\")\n",
        "\n",
        "def generate_insights(ngram_frequencies, ngram_type):\n",
        "\n",
        "    sorted_ngrams = sorted(ngram_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(f\"\\nTop 5 {ngram_type}s:\")\n",
        "    for ngram, freq in sorted_ngrams[:5]:\n",
        "        print(f\" - {ngram}: {freq}\")\n",
        "\n",
        "bigram_frequencies_yoda = count_ngrams(yoda_bigrams)\n",
        "trigram_frequencies_yoda = count_ngrams(yoda_bigrams)\n",
        "\n",
        "print(\"\\nYoda's Speech Insights:\")\n",
        "generate_insights(bigram_frequencies_yoda, \"bigram\")\n",
        "generate_insights(trigram_frequencies_yoda, \"trigram\")\n",
        "\n",
        "print(\"\\nStandard English Insights:\")\n",
        "generate_insights(standard_bigram_frequencies, \"bigram\")\n",
        "generate_insights(standard_trigram_frequencies, \"trigram\")\n",
        "\n",
        "\n",
        "def print_top_ngrams(ngram_frequencies, ngram_type, top_n=5):\n",
        "    sorted_ngrams = sorted(ngram_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(f\"\\nTop {top_n} {ngram_type}s:\")\n",
        "    for ngram, freq in sorted_ngrams[:top_n]:\n",
        "        print(f\" - {ngram}: {freq}\")\n",
        "\n",
        "print(\"Top Bigrams in Yoda's Speech:\")\n",
        "print_top_ngrams(bigram_frequencies_yoda, \"bigram\")\n",
        "\n",
        "print(\"Top Trigrams in Yoda's Speech:\")\n",
        "print_top_ngrams(trigram_frequencies_yoda, \"trigram\")\n",
        "\n",
        "print(\"Top Bigrams in Standard English:\")\n",
        "print_top_ngrams(standard_bigram_frequencies, \"bigram\")\n",
        "\n",
        "print(\"Top Trigrams in Standard English:\")\n",
        "print_top_ngrams(standard_trigram_frequencies, \"trigram\")\n",
        "\n",
        "\n",
        "def display_most_frequent_ngrams(ngram_freq, n=5):\n",
        "    # Sort n-grams by frequency\n",
        "    sorted_ngrams = sorted(ngram_freq.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Display top n most frequent n-grams\n",
        "    print(f\"Top {n} most frequent n-grams:\")\n",
        "    for ngram, freq in sorted_ngrams[:n]:\n",
        "        print(f\"{ngram}: {freq}\")\n",
        "\n",
        "print(\"Most Frequent Bigrams:\")\n",
        "display_most_frequent_ngrams(bigram_frequencies)\n",
        "\n",
        "print(\"\\nMost Frequent Trigrams:\")\n",
        "display_most_frequent_ngrams(trigram_frequencies)\n",
        "\n",
        "next_word = predict_next_word(\"Much to\", trigram_frequencies) # Pass the trigram_frequencies to the function\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prniGMR63bNt",
        "outputId": "4649c842-9819-40b3-c25c-81ff25e5b876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  yoda-corpus.csv\n",
            "Header: ['1', '129', '1158', 'narrator', '\"QUI-GON stands in a tall stately room. Twelve JEDI sit in a semi-circle. OBI-WAN stands behind QUI-GON in the center of the room. The Senior Jedi is MACE WINDU. To his left is an alien Jedi named KI-ADI-MUNDI', ' and to his right', ' the Jedi Master', ' YODA.\"', 'INT. TEMPLE OF THE JEDI - COUNCIL CHAMBERS - DAY', 'action']\n",
            "Data sample: [['1', '129', '1159', 'QUI-GON', '...my only conclusion can be that it was a Sith Lord.', 'INT. TEMPLE OF THE JEDI - COUNCIL CHAMBERS - DAY', 'character'], ['1', '129', '1160', 'MACE WINDU', 'A Sith Lord?!?', 'INT. TEMPLE OF THE JEDI - COUNCIL CHAMBERS - DAY', 'character'], ['1', '129', '1161', 'KI-ADI', 'Impossible! The Sith have been extinct for a millenium.', 'INT. TEMPLE OF THE JEDI - COUNCIL CHAMBERS - DAY', 'character'], ['1', '129', '1162', 'YODA', '\"The very Republic is threatened', ' if involved the Sith are.\"', 'INT. TEMPLE OF THE JEDI - COUNCIL CHAMBERS - DAY', 'character'], ['1', '129', '1163', 'MACE WINDU', 'I do not believe they could have returned without us knowing.', 'INT. TEMPLE OF THE JEDI - COUNCIL CHAMBERS - DAY', 'character']]\n",
            "Error: File 'yoda_corpus.csv' not found. please check the file path\n",
            "[]\n",
            "Error: 'text' column not found in the header.\n",
            "Comparing Bigrams:\n",
            "Unique bigrams in the first set:\n",
            " - help you: 1\n",
            " - you i: 1\n",
            " - i can: 1\n",
            " - can yes: 1\n",
            " - the force: 1\n",
            " - force is: 1\n",
            " - is strong: 1\n",
            " - strong with: 1\n",
            " - with you: 1\n",
            " - pass on: 1\n",
            " - on what: 1\n",
            " - much to: 1\n",
            " - to learn: 1\n",
            " - learn you: 1\n",
            " - you still: 1\n",
            " - still have: 1\n",
            "\n",
            "Unique bigrams in the second set:\n",
            " - you must: 1\n",
            " - must unlearn: 1\n",
            " - unlearn what: 1\n",
            " - do or: 1\n",
            " - or do: 1\n",
            " - do not: 1\n",
            " - not there: 1\n",
            " - there is: 1\n",
            " - is no: 1\n",
            " - no try: 1\n",
            " - the greatest: 1\n",
            " - greatest teacher: 1\n",
            " - teacher failure: 1\n",
            " - failure is: 1\n",
            " - adventure excitement: 1\n",
            " - excitement a: 1\n",
            " - a jedi: 1\n",
            " - jedi craves: 1\n",
            " - craves not: 1\n",
            " - not these: 1\n",
            " - these things: 1\n",
            " - fear is: 1\n",
            " - is the: 1\n",
            " - the path: 1\n",
            " - path to: 1\n",
            " - to the: 1\n",
            " - the dark: 1\n",
            " - dark side: 1\n",
            "\n",
            "Comparing Trigrams:\n",
            "Unique trigrams in the first set:\n",
            " - help you i: 1\n",
            " - you i can: 1\n",
            " - i can yes: 1\n",
            " - the force is: 1\n",
            " - force is strong: 1\n",
            " - is strong with: 1\n",
            " - strong with you: 1\n",
            " - pass on what: 1\n",
            " - on what you: 1\n",
            " - much to learn: 1\n",
            " - to learn you: 1\n",
            " - learn you still: 1\n",
            " - you still have: 1\n",
            "\n",
            "Unique trigrams in the second set:\n",
            " - you must unlearn: 1\n",
            " - must unlearn what: 1\n",
            " - unlearn what you: 1\n",
            " - do or do: 1\n",
            " - or do not: 1\n",
            " - do not there: 1\n",
            " - not there is: 1\n",
            " - there is no: 1\n",
            " - is no try: 1\n",
            " - the greatest teacher: 1\n",
            " - greatest teacher failure: 1\n",
            " - teacher failure is: 1\n",
            " - adventure excitement a: 1\n",
            " - excitement a jedi: 1\n",
            " - a jedi craves: 1\n",
            " - jedi craves not: 1\n",
            " - craves not these: 1\n",
            " - not these things: 1\n",
            " - fear is the: 1\n",
            " - is the path: 1\n",
            " - the path to: 1\n",
            " - path to the: 1\n",
            " - to the dark: 1\n",
            " - the dark side: 1\n",
            "\n",
            "Yoda's Speech Insights:\n",
            "\n",
            "Top 5 bigrams:\n",
            " - help you: 1\n",
            " - you i: 1\n",
            " - i can: 1\n",
            " - can yes: 1\n",
            " - the force: 1\n",
            "\n",
            "Top 5 trigrams:\n",
            " - help you: 1\n",
            " - you i: 1\n",
            " - i can: 1\n",
            " - can yes: 1\n",
            " - the force: 1\n",
            "\n",
            "Standard English Insights:\n",
            "\n",
            "Top 5 bigrams:\n",
            " - you must: 1\n",
            " - must unlearn: 1\n",
            " - unlearn what: 1\n",
            " - what you: 1\n",
            " - you have: 1\n",
            "\n",
            "Top 5 trigrams:\n",
            " - you must unlearn: 1\n",
            " - must unlearn what: 1\n",
            " - unlearn what you: 1\n",
            " - what you have: 1\n",
            " - you have learned: 1\n",
            "Top Bigrams in Yoda's Speech:\n",
            "\n",
            "Top 5 bigrams:\n",
            " - help you: 1\n",
            " - you i: 1\n",
            " - i can: 1\n",
            " - can yes: 1\n",
            " - the force: 1\n",
            "Top Trigrams in Yoda's Speech:\n",
            "\n",
            "Top 5 trigrams:\n",
            " - help you: 1\n",
            " - you i: 1\n",
            " - i can: 1\n",
            " - can yes: 1\n",
            " - the force: 1\n",
            "Top Bigrams in Standard English:\n",
            "\n",
            "Top 5 bigrams:\n",
            " - you must: 1\n",
            " - must unlearn: 1\n",
            " - unlearn what: 1\n",
            " - what you: 1\n",
            " - you have: 1\n",
            "Top Trigrams in Standard English:\n",
            "\n",
            "Top 5 trigrams:\n",
            " - you must unlearn: 1\n",
            " - must unlearn what: 1\n",
            " - unlearn what you: 1\n",
            " - what you have: 1\n",
            " - you have learned: 1\n",
            "Most Frequent Bigrams:\n",
            "Top 5 most frequent n-grams:\n",
            "help you: 1\n",
            "you i: 1\n",
            "i can: 1\n",
            "can yes: 1\n",
            "the force: 1\n",
            "\n",
            "Most Frequent Trigrams:\n",
            "Top 5 most frequent n-grams:\n",
            "help you i: 1\n",
            "you i can: 1\n",
            "i can yes: 1\n",
            "the force is: 1\n",
            "force is strong: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    \"\"\"Tokenize the sentence into words.\"\"\"\n",
        "    return sentence.split()\n",
        "\n",
        "def simple_pos_tag(tokens):\n",
        "    \"\"\"Simplistically tag each token with a part of speech.\"\"\"\n",
        "    pos_tags = []\n",
        "    for token in tokens:\n",
        "        if token.lower() in {\"much\", \"you\", \"have\", \"the\", \"is\", \"are\", \"it\"}:\n",
        "            pos_tags.append((token, \"PRON\"))  # Pronouns\n",
        "        elif token.lower() in {\"to\", \"learn\", \"teaches\", \"threatened\", \"involved\", \"discover\", \"must\"}:\n",
        "            pos_tags.append((token, \"VERB\"))  # Verbs\n",
        "        elif token.lower() in {\"still\", \"hard\", \"dark\", \"very\", \"much\", \"that\"}:\n",
        "            pos_tags.append((token, \"ADV\"))  # Adverbs\n",
        "        elif token.lower() in {\"jedi\", \"master\", \"yoda\", \"luke\", \"skywalker\", \"republic\", \"sith\", \"side\", \"assassin\"}:\n",
        "            pos_tags.append((token, \"NOUN\"))  # Nouns\n",
        "        else:\n",
        "            pos_tags.append((token, \"UNKNOWN\"))  # Unknown parts of speech\n",
        "    return pos_tags\n",
        "\n",
        "def dependency_analysis(pos_tags):\n",
        "    dependencies = []\n",
        "    for i, (token, pos) in enumerate(pos_tags):\n",
        "        if pos == \"VERB\":\n",
        "            # Assume the verb is related to the preceding pronoun or noun\n",
        "            if i > 0 and pos_tags[i - 1][1] in {\"PRON\", \"NOUN\"}:\n",
        "                dependencies.append((pos_tags[i - 1][0], token))\n",
        "        elif pos == \"PRON\":\n",
        "            # Pronouns typically relate to verbs\n",
        "            if i < len(pos_tags) - 1 and pos_tags[i + 1][1] == \"VERB\":\n",
        "                dependencies.append((token, pos_tags[i + 1][0]))\n",
        "    return dependencies\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"The Jedi Master Yoda teaches Luke Skywalker.\",\n",
        "    \"The very Republic is threatened, if involved the Sith are.\",\n",
        "    \"Hard to see, the dark side is. Discover who this assassin is, we must.\",\n",
        "    \"Much to learn you still have.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"\\nProcessing sentence: '{sentence}'\")\n",
        "\n",
        "\n",
        "    tokens = tokenize(sentence)\n",
        "    print(\"Tokens:\", tokens)\n",
        "\n",
        "    # Perform POS tagging\n",
        "    pos_tags = simple_pos_tag(tokens)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "    # Perform dependency analysis\n",
        "    dependencies = dependency_analysis(pos_tags)\n",
        "    print(\"Dependencies:\", dependencies)\n",
        "\n"
      ],
      "metadata": {
        "id": "lOBfkH7grz3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b4f7fd-081a-4454-a844-369939a4b3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing sentence: 'The Jedi Master Yoda teaches Luke Skywalker.'\n",
            "Tokens: ['The', 'Jedi', 'Master', 'Yoda', 'teaches', 'Luke', 'Skywalker.']\n",
            "POS Tags: [('The', 'PRON'), ('Jedi', 'NOUN'), ('Master', 'NOUN'), ('Yoda', 'NOUN'), ('teaches', 'VERB'), ('Luke', 'NOUN'), ('Skywalker.', 'UNKNOWN')]\n",
            "Dependencies: [('Yoda', 'teaches')]\n",
            "\n",
            "Processing sentence: 'The very Republic is threatened, if involved the Sith are.'\n",
            "Tokens: ['The', 'very', 'Republic', 'is', 'threatened,', 'if', 'involved', 'the', 'Sith', 'are.']\n",
            "POS Tags: [('The', 'PRON'), ('very', 'ADV'), ('Republic', 'NOUN'), ('is', 'PRON'), ('threatened,', 'UNKNOWN'), ('if', 'UNKNOWN'), ('involved', 'VERB'), ('the', 'PRON'), ('Sith', 'NOUN'), ('are.', 'UNKNOWN')]\n",
            "Dependencies: []\n",
            "\n",
            "Processing sentence: 'Hard to see, the dark side is. Discover who this assassin is, we must.'\n",
            "Tokens: ['Hard', 'to', 'see,', 'the', 'dark', 'side', 'is.', 'Discover', 'who', 'this', 'assassin', 'is,', 'we', 'must.']\n",
            "POS Tags: [('Hard', 'ADV'), ('to', 'VERB'), ('see,', 'UNKNOWN'), ('the', 'PRON'), ('dark', 'ADV'), ('side', 'NOUN'), ('is.', 'UNKNOWN'), ('Discover', 'VERB'), ('who', 'UNKNOWN'), ('this', 'UNKNOWN'), ('assassin', 'NOUN'), ('is,', 'UNKNOWN'), ('we', 'UNKNOWN'), ('must.', 'UNKNOWN')]\n",
            "Dependencies: []\n",
            "\n",
            "Processing sentence: 'Much to learn you still have.'\n",
            "Tokens: ['Much', 'to', 'learn', 'you', 'still', 'have.']\n",
            "POS Tags: [('Much', 'PRON'), ('to', 'VERB'), ('learn', 'VERB'), ('you', 'PRON'), ('still', 'ADV'), ('have.', 'UNKNOWN')]\n",
            "Dependencies: [('Much', 'to'), ('Much', 'to')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "word_embeddings = {\n",
        "    \"the\": [0.1, 0.2, 0.3],\n",
        "    \"jedi\": [0.4, 0.5, 0.6],\n",
        "    \"master\": [0.7, 0.8, 0.9],\n",
        "    \"yoda\": [0.2, 0.1, 0.4],\n",
        "    \"teaches\": [0.3, 0.7, 0.5],\n",
        "    \"luke\": [0.6, 0.3, 0.1],\n",
        "    \"skywalker\": [0.5, 0.9, 0.4],\n",
        "    \"republic\": [0.8, 0.1, 0.3],\n",
        "    \"threatened\": [0.2, 0.6, 0.7],\n",
        "    \"sith\": [0.4, 0.5, 0.9],\n",
        "    \"hard\": [0.3, 0.2, 0.6],\n",
        "    \"dark\": [0.8, 0.4, 0.5],\n",
        "    \"side\": [0.5, 0.6, 0.7],\n",
        "    \"discover\": [0.7, 0.2, 0.4],\n",
        "    \"assassin\": [0.4, 0.8, 0.3],\n",
        "    \"much\": [0.6, 0.1, 0.4],\n",
        "    \"learn\": [0.2, 0.7, 0.5],\n",
        "    \"still\": [0.3, 0.5, 0.6],\n",
        "    \"have\": [0.8, 0.4, 0.7]\n",
        "}\n",
        "\n",
        "# Convert sentence to vector by averaging word embeddings\n",
        "def sentence_to_vector(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = [word_embeddings.get(word, [0, 0, 0]) for word in words]\n",
        "\n",
        "    if len(vectors) == 0:\n",
        "        return [0, 0, 0]\n",
        "\n",
        "    # Compute the average vector\n",
        "    avg_vector = [sum(v) / len(vectors) for v in zip(*vectors)]\n",
        "    return avg_vector\n",
        "\n",
        "# Compute cosine similarity between two vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(x * y for x, y in zip(vec1, vec2))\n",
        "    norm1 = sum(x * x for x in vec1) ** 0.5\n",
        "    norm2 = sum(y * y for y in vec2) ** 0.5\n",
        "\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0\n",
        "\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"The Jedi Master Yoda teaches Luke Skywalker.\",\n",
        "    \"The very Republic is threatened, if involved the Sith are.\",\n",
        "    \"Hard to see, the dark side is. Discover who this assassin is, we must.\",\n",
        "    \"Much to learn you still have.\"\n",
        "]\n",
        "\n",
        "# Convert sentences to vectors\n",
        "sentence_vectors = [sentence_to_vector(sentence) for sentence in sentences]\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i + 1, len(sentences)):\n",
        "        sim = cosine_similarity(sentence_vectors[i], sentence_vectors[j])\n",
        "        print(f\"Similarity between sentence {i + 1} and sentence {j + 1}: {sim:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYKwebQvBs4H",
        "outputId": "e7454793-b527-4977-920b-692172c864f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between sentence 1 and sentence 2: 0.98\n",
            "Similarity between sentence 1 and sentence 3: 0.99\n",
            "Similarity between sentence 1 and sentence 4: 1.00\n",
            "Similarity between sentence 2 and sentence 3: 0.99\n",
            "Similarity between sentence 2 and sentence 4: 0.98\n",
            "Similarity between sentence 3 and sentence 4: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentiment scores for words\n",
        "sentiment_scores = {\n",
        "    \"happy\": 1,\n",
        "    \"joyful\": 1,\n",
        "    \"sad\": -1,\n",
        "    \"angry\": -1,\n",
        "    \"neutral\": 0,\n",
        "    \"excited\": 1,\n",
        "    \"bored\": -1,\n",
        "    \"learn\": 0,\n",
        "    \"you\": 0,\n",
        "    \"still\": 0,\n",
        "    \"have\": 0,\n",
        "    \"much\": 0,\n",
        "    \"to\": 0,\n",
        "    \"the\": 0,\n",
        "    \"dark\": -1,\n",
        "    \"side\": -1,\n",
        "    \"is\": 0,\n",
        "    \"a\": 0,\n",
        "    \"jedi\": 0,\n",
        "    \"craves\": 0,\n",
        "    \"not\": -1,\n",
        "    \"these\": 0,\n",
        "    \"things\": 0,\n",
        "    \"fear\": -1,\n",
        "    \"path\": -1,\n",
        "    \"the\": 0,\n",
        "    \"you\": 0,\n",
        "    \"have\": 0,\n",
        "    \"learned\": 0,\n",
        "    \"must\": 0,\n",
        "    \"unlearn\": -1,\n",
        "    \"what\": 0,\n",
        "    \"do\": 0,\n",
        "    \"or\": 0,\n",
        "    \"try\": -1,\n",
        "    \"greatest\": 1,\n",
        "    \"teacher\": 1,\n",
        "    \"failure\": -1,\n",
        "    \"adventure\": 1,\n",
        "    \"excitement\": 1,\n",
        "    \"jedi\": 0\n",
        "}\n",
        "\n",
        "# Analyze sentiment of a sentence\n",
        "def analyze_sentiment(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    score = sum(sentiment_scores.get(word, 0) for word in words)\n",
        "\n",
        "    if score > 0:\n",
        "        return \"Positive\"\n",
        "    elif score < 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "#  Yoda's speech sentences\n",
        "sentences = [\n",
        "    \"Much to learn you still have.\",\n",
        "    \"Fear is the path to the dark side.\",\n",
        "    \"You must unlearn what you have learned.\",\n",
        "    \"The greatest teacher, failure is.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"Sentence: '{sentence}' - Sentiment: {analyze_sentiment(sentence)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA8Mgb-NDvgl",
        "outputId": "052088fe-e0e0-482a-950b-efc711f73470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: 'Much to learn you still have.' - Sentiment: Neutral\n",
            "Sentence: 'Fear is the path to the dark side.' - Sentiment: Negative\n",
            "Sentence: 'You must unlearn what you have learned.' - Sentiment: Negative\n",
            "Sentence: 'The greatest teacher, failure is.' - Sentiment: Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple translation dictionaries\n",
        "translations = {\n",
        "    \"much\": {\"fr\": \"beaucoup\", \"es\": \"mucho\", \"de\": \"viel\"},\n",
        "    \"to\": {\"fr\": \"à\", \"es\": \"a\", \"de\": \"zu\"},\n",
        "    \"learn\": {\"fr\": \"apprendre\", \"es\": \"aprender\", \"de\": \"lernen\"},\n",
        "    \"you\": {\"fr\": \"vous\", \"es\": \"usted\", \"de\": \"du\"},\n",
        "    \"still\": {\"fr\": \"encore\", \"es\": \"aún\", \"de\": \"noch\"},\n",
        "    \"have\": {\"fr\": \"avoir\", \"es\": \"tener\", \"de\": \"haben\"},\n",
        "    \"fear\": {\"fr\": \"peur\", \"es\": \"miedo\", \"de\": \"Angst\"},\n",
        "    \"is\": {\"fr\": \"est\", \"es\": \"es\", \"de\": \"ist\"},\n",
        "    \"the\": {\"fr\": \"le\", \"es\": \"el\", \"de\": \"der\"},\n",
        "    \"path\": {\"fr\": \"chemin\", \"es\": \"camino\", \"de\": \"Pfad\"},\n",
        "    \"dark\": {\"fr\": \"sombre\", \"es\": \"oscuro\", \"de\": \"dunkel\"},\n",
        "    \"side\": {\"fr\": \"côté\", \"es\": \"lado\", \"de\": \"Seite\"},\n",
        "    \"must\": {\"fr\": \"doit\", \"es\": \"debe\", \"de\": \"muss\"},\n",
        "    \"unlearn\": {\"fr\": \"désapprendre\", \"es\": \"desaprender\", \"de\": \"verlernen\"},\n",
        "    \"what\": {\"fr\": \"ce que\", \"es\": \"lo que\", \"de\": \"was\"},\n",
        "    \"learned\": {\"fr\": \"appris\", \"es\": \"aprendido\", \"de\": \"gelernt\"},\n",
        "    \"greatest\": {\"fr\": \"le plus grand\", \"es\": \"el mayor\", \"de\": \"größten\"},\n",
        "    \"teacher\": {\"fr\": \"enseignant\", \"es\": \"maestro\", \"de\": \"Lehrer\"},\n",
        "    \"failure\": {\"fr\": \"échec\", \"es\": \"fracaso\", \"de\": \"Fehlschlag\"},\n",
        "    \"adventure\": {\"fr\": \"aventure\", \"es\": \"aventura\", \"de\": \"Abenteuer\"},\n",
        "    \"excitement\": {\"fr\": \"excitation\", \"es\": \"emoción\", \"de\": \"Aufregung\"},\n",
        "    \"jedi\": {\"fr\": \"jedi\", \"es\": \"jedi\", \"de\": \"jedi\"}\n",
        "}\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, language):\n",
        "    words = sentence.lower().split()\n",
        "    translated_words = [translations.get(word, {}).get(language, word) for word in words]\n",
        "    return \" \".join(translated_words)\n",
        "\n",
        "# Yoda's speech sentences\n",
        "sentences = [\n",
        "    \"Much to learn you still have.\",\n",
        "    \"Fear is the path to the dark side.\",\n",
        "    \"You must unlearn what you have learned.\",\n",
        "    \"The greatest teacher, failure is.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"Original: '{sentence}'\")\n",
        "    print(f\"French: '{translate_sentence(sentence, 'fr')}'\")\n",
        "    print(f\"Spanish: '{translate_sentence(sentence, 'es')}'\")\n",
        "    print(f\"German: '{translate_sentence(sentence, 'de')}'\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx0WX3yLIVFf",
        "outputId": "4f74d28d-39cf-4f28-97fc-d8e74d5cdd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 'Much to learn you still have.'\n",
            "French: 'beaucoup à apprendre vous encore have.'\n",
            "Spanish: 'mucho a aprender usted aún have.'\n",
            "German: 'viel zu lernen du noch have.'\n",
            "\n",
            "Original: 'Fear is the path to the dark side.'\n",
            "French: 'peur est le chemin à le sombre side.'\n",
            "Spanish: 'miedo es el camino a el oscuro side.'\n",
            "German: 'Angst ist der Pfad zu der dunkel side.'\n",
            "\n",
            "Original: 'You must unlearn what you have learned.'\n",
            "French: 'vous doit désapprendre ce que vous avoir learned.'\n",
            "Spanish: 'usted debe desaprender lo que usted tener learned.'\n",
            "German: 'du muss verlernen was du haben learned.'\n",
            "\n",
            "Original: 'The greatest teacher, failure is.'\n",
            "French: 'le le plus grand teacher, échec is.'\n",
            "Spanish: 'el el mayor teacher, fracaso is.'\n",
            "German: 'der größten teacher, Fehlschlag is.'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram  Yoda's speech\n",
        "bigram_model = {\n",
        "    (\"much\", \"to\"): \"learn\",\n",
        "    (\"to\", \"learn\"): \"you\",\n",
        "    (\"learn\", \"you\"): \"still\",\n",
        "    (\"you\", \"still\"): \"have\",\n",
        "    (\"fear\", \"is\"): \"the\",\n",
        "    (\"is\", \"the\"): \"path\",\n",
        "    (\"the\", \"path\"): \"to\",\n",
        "    (\"path\", \"to\"): \"the\",\n",
        "    (\"to\", \"the\"): \"dark\",\n",
        "    (\"the\", \"dark\"): \"side\",\n",
        "    (\"you\", \"must\"): \"unlearn\",\n",
        "    (\"must\", \"unlearn\"): \"what\",\n",
        "    (\"unlearn\", \"what\"): \"you\",\n",
        "    (\"what\", \"you\"): \"have\",\n",
        "    (\"you\", \"have\"): \"learned\",\n",
        "    (\"greatest\", \"teacher\"): \"failure\",\n",
        "    (\"teacher\", \"failure\"): \"is\",\n",
        "    (\"failure\", \"is\"): \"the\",\n",
        "    (\"adventure\", \"excitement\"): \"a\",\n",
        "    (\"excitement\", \"a\"): \"jedi\",\n",
        "    (\"a\", \"jedi\"): \"craves\",\n",
        "}\n",
        "\n",
        "# Predict the next word\n",
        "def predict_next_word_bigrams(previous_words, bigram_model):\n",
        "    previous_words_tuple = tuple(previous_words.split()[-2:])\n",
        "    return bigram_model.get(previous_words_tuple, \"Unknown\")\n",
        "\n",
        "# Test\n",
        "sentence_start = \"much to\"\n",
        "next_word = predict_next_word_bigrams(sentence_start, bigram_model)\n",
        "print(f\"Given '{sentence_start}', the next word is predicted to be '{next_word}'.\")\n",
        "\n",
        "sentence_start = \"you still\"\n",
        "next_word = predict_next_word_bigrams(sentence_start, bigram_model)\n",
        "print(f\"Given '{sentence_start}', the next word is predicted to be '{next_word}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD-tbsVJPBya",
        "outputId": "4c2000d4-5a08-4e16-bdc3-c8173cda1e99"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given 'much to', the next word is predicted to be 'learn'.\n",
            "Given 'you still', the next word is predicted to be 'have'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_bigrams(corpus):\n",
        "    bigrams = {}\n",
        "    words = corpus.lower().split()\n",
        "\n",
        "    for i in range(len(words) - 1):\n",
        "        pair = (words[i], words[i + 1])\n",
        "        if pair[0] in bigrams:\n",
        "            if pair[1] in bigrams[pair[0]]:\n",
        "                bigrams[pair[0]][pair[1]] += 1\n",
        "            else:\n",
        "                bigrams[pair[0]][pair[1]] = 1\n",
        "        else:\n",
        "            bigrams[pair[0]] = {pair[1]: 1}\n",
        "\n",
        "    return bigrams\n",
        "\n",
        "#  Predict the next word based on the current word\n",
        "def predict_next_word(current_word, bigrams):\n",
        "    current_word = current_word.lower()\n",
        "    if current_word in bigrams:\n",
        "        next_word = max(bigrams[current_word], key=bigrams[current_word].get)\n",
        "        return next_word\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Yoda corpus\n",
        "yoda_corpus = \"\"\"\n",
        "much to learn you still have.\n",
        "the force is strong with you.\n",
        "help you i can, yes.\n",
        "do or do not, there is no try.\n",
        "\"\"\"\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams = generate_bigrams(yoda_corpus)\n",
        "\n",
        "# Take input from the user\n",
        "current_word = input(\"Enter a word to predict the next word: \")\n",
        "\n",
        "# Predict the next word\n",
        "next_word = predict_next_word(current_word, bigrams)\n",
        "\n",
        "if next_word:\n",
        "    print(f\"Given the word '{current_word}', the predicted next word is '{next_word}'.\")\n",
        "else:\n",
        "    print(f\"No prediction available for the word '{current_word}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dSnmxwxP2pn",
        "outputId": "838f1d80-72dc-4a8f-bdf0-7867d26bcfcb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word to predict the next word: much\n",
            "Given the word 'much', the predicted next word is 'to'.\n"
          ]
        }
      ]
    }
  ]
}